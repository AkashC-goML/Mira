{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file name\n",
    "file_name = 'aw_fb_data.csv'\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data = pd.read_csv(file_name)\n",
    "    print(f\"Data loaded successfully. Shape: {data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {file_name} not found in the current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Handling missing values\n",
    "# For numerical columns, we'll use median imputation\n",
    "numerical_cols = ['age', 'height', 'weight', 'steps', 'heart_rate', 'calories', 'distance']\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "data[numerical_cols] = numerical_imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "# For categorical columns, we'll use the most frequent strategy\n",
    "categorical_cols = ['gender']\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[categorical_cols] = categorical_imputer.fit_transform(data[categorical_cols])\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\\nMissing Values after Imputation:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return filtered_df\n",
    "\n",
    "# Apply outlier removal to numerical columns\n",
    "for col in numerical_cols:\n",
    "    before = data.shape[0]\n",
    "    data = remove_outliers(data, col)\n",
    "    after = data.shape[0]\n",
    "    print(f\"Removed {before - after} outliers from {col}\")\n",
    "\n",
    "print(f\"\\nData shape after outlier removal: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'gender' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['gender'] = label_encoder.fit_transform(data['gender'])\n",
    "\n",
    "# Display unique values after encoding\n",
    "print(\"Encoded 'gender' values:\", data['gender'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "print(\"\\nData after scaling:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of steps\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data['steps'], bins=30, kde=True)\n",
    "plt.title('Distribution of Steps')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot between heart rate and steps\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x='steps', y='heart_rate', hue='gender', data=data)\n",
    "plt.title('Heart Rate vs Steps by Gender')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of calories by gender\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(x='gender', y='calories', data=data)\n",
    "plt.title('Calories Burned by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: BMI calculation\n",
    "# Since height is in standardized form, let's assume original height was in cm\n",
    "# To calculate BMI = weight (kg) / (height (m))^2\n",
    "# We'll reverse the scaling for height and weight temporarily\n",
    "\n",
    "# Inverse transform height and weight\n",
    "height_scaled = data['height']\n",
    "weight_scaled = data['weight']\n",
    "\n",
    "# Assuming mean=0 and std=1 for standardized data\n",
    "# BMI = (weight * std_weight + mean_weight) / ((height * std_height + mean_height)/100)^2\n",
    "# For simplicity, we'll skip inverse scaling and treat BMI as a derived feature in scaled form\n",
    "data['BMI'] = weight_scaled / (height_scaled ** 2)\n",
    "\n",
    "# Display the new feature\n",
    "print(data[['BMI']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "# Let's assume heart_rate > 100 (after inverse scaling) is abnormal\n",
    "# Since data is scaled, we'll set a threshold accordingly\n",
    "# For simplicity, we'll convert it back to original scale approximately\n",
    "# Assuming mean heart rate is around 70 bpm and std is around 10\n",
    "\n",
    "# Define abnormal heart rate as scaled > (100 - 70)/10 = 3\n",
    "data['abnormal_heart_rate'] = data['heart_rate'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "print(data['abnormal_heart_rate'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = data.drop(['abnormal_heart_rate'], axis=1)\n",
    "y = data['abnormal_heart_rate']\n",
    "\n",
    "# Splitting into train (70%), validation (15%), and test (15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42, probability=True),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print(f\"{name} Validation Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume Random Forest performed the best\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                           cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
